llm:
  provider: openai
  config:
    model: MiniMax-M2.5
    temperature: 0.1
    openai_base_url: https://api.minimax.io/v1
    api_key: ${MINIMAX_API_KEY}

embedder:
  provider: ollama
  config:
    model: bge-m3
    embedding_dims: 1024

vector_store:
  provider: qdrant
  config:
    host: localhost
    port: 6333
    embedding_model_dims: 1024
    collection_name: agentMemory

# 自定义事实提取 prompt 文件路径 (相对于 backend 目录)
custom_fact_extraction_prompt: prompts/project_memory_prompt.txt
